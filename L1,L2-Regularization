릿지 회귀(Ridge) - L2
릿지 회귀에서의 가중치(w) 선택은 훈련 데이터를 잘 예측하기 위한 하나와 추가 제약 조건을 만족시키기 위한 목적
가중치의 절대값을 가능한 작게 만든다. 
w(기울기)의 모든 값이 0에 가깝게 되길 원한다. 
모든 특성이 출력에 주는 영향을 작게 만든다.

규제(Regularization)을 하는 이유
 : 과대적합(Overfitting)이 되지 않도록 모델을 강제로 제한한다.
릿지 회귀의 규제 방식은 L2규제라고하기도 한다.
릿지 회귀는 linear_model.Ridge에 구현되어 있음.

선형회귀는 과대적합, Ridge는 규제로 인해 과대적합이 적어진다.
alpha을 이용하여 훈련세트의 성능 대비 모델을 얼마나 단순화 시킬 수 있는지 지정 가능.(기본값 alpha=1.0)
alpha의 계수를 높이면 w의 계수를 0에 가깝게 만든다. 
계수가 0에 가까워지면 일반화에 도움이 된다.(훈련세트 성능이 나빠짐)
alpha의 게수를 줄이면 그만큼 풀리면서 LinearRegression 으로 모델과 점점 가까워짐

mglearn을 이용하여 훈련 데이터의 크기를 변화시키며 학습 곡선 확인해 볼 수 있다.
alpha = 커질수록 0에 가까워짐

모든 데이터 셋에서 TRAINING의 데이터 셋으로 모델 생성의 경우, 일반선형회귀의 r^2의 값이 높다.
단, 테스트 데이터 셋에서는 모델은 비교시에 릿지 회귀가 Score(r^2)의 값이 높다.
테스트 셋에 대한 경우, 데이터가 많아지면 선형회귀 모델이 릿지 모델에 Score가 거의 가까워진다.
충분한 데이터는 규제항이 덜 중요해져 릿지 회귀와 선형회귀는 같아진다.


라쏘 회귀(Lasso) -릿지(Ridge)의 대안 (L1규제)
릿지 회귀에서와 같이 w(가중치-계수)의 모든 원소가 0에 가깝게 되길 원한다.(규제)
릿지 회귀와 달리 라쏘(Lasso)는 실제로 어떤 계수를 0으로 만든다.- 완전히 제외되는 특성이 발생
라쏘 회귀의 규제 방식은 L1규제라고 하기도 한다.

alpha가 적어지면 적어질수록 규제를 받지 않는 모델이 된다.
실제의 경우는 보통 릿지 회귀를 선호함.
만약 특성이 많고 일부부만 중요하다면 Lasso가 좋은 선택
