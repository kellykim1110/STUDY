베이지안 딥러닝을 공부하기 위해서는 확률을 이해하고 있어야 한다.

조건부 확률에서 일반적인 이론으로 다음과 같다.
x가 주어졌을 때 y가 나올 확률 
p(x|y) = p(y|x) p(x) / p(y)
p(x|y) = p(x ^ y) / p(y)

하지만 다음 확률이 Gaussian에서 나온건지 Bionomial에서 나온건지 알 수 없기 때문에 X, Y로 구분하여 표현하는게 정확하다.
Random variables Don't help
 : PX|Y(x|y) = PY|X(y|x) pX(x) / pY(y)


### 우리가 앞으로 항상 생각해야 하는 것
확률이 무엇인지 ( What is probability? )
확률 변수가 무엇인지 ( What is Random Variable? )
가우시안 프로세스(랜덤변수의 집합)이 무엇인지 ( What is Gaussian Process? )
커널 함수가 무엇인지 ( What is kernal function? )


확률이 무엇인지 알아보기 위해 우선 시그마 필드에 대해 얘기 해 보겠다.

시그마 필드( B )란 다음 세 개의 공리가 성립하는 전체집합의 부분집합 콜렉션( a collection of subsets of U ) 을 의미한다.
1. 공집합 ∮ ∈ B ( empty set is included )
2.  A∈B ⇒ (U-A)∈B ( closed under set complement )
3.  Ai ∈ B ⇒ ∪∞,i=1 Ai  ∈ B ( closed under countable union )

시그마 필드는 측도를 정의하기 위해 지정되었다. ( A sigma-field is designed to drfine a measure )
어떤 원소가 시그마 필드에 포함되지 않으면 그 원소는 측정되지 않는다. ( If the element is not inside a sigma-field, it cannot be measured )

따라서, 다음과 같이 정의 할 수 있다.
A set U and a lsigma-field B of subsets of U form a measurable space (U, B).
A measure M defined on measureable space (U, B) is a set function M : B -> [0, ∞] such that 
1. M(∮) =0
2. For disjoint Bi and Bj ( i.e. Bi ∩ Bj =∮ ) ⇒ M(∪∞,i=1 Bi) = ∑∞,i=1 M(Bi) (countable additivity)

#결론
Probability is a measure such that M(U) = 1, i.e. normalized measure.


