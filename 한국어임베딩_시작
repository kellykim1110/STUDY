자연어 처리 분야에서 임베딩이란 
사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로  바꾼결과 혹은 그 일련의 과정 전체를 의미

가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것
단어-문서행렬 : 행은 단어, 열은 문서에 대응하는 행렬

임베딩의 역할
1. 단어/문장 간 관련도 계산
Word2Vec : 구글이 개발한 단어들을 벡터로 바꾸는 방법
코사인 유사도 : 각 퀘리 단어별로 벡터 간 유사도 측정 기법의 일종
=> 이를 통해 단어/문장 간 관련도를 계산할 수 있다.
t-SNE : 차원축소기법으로 큰차원을 더 작은 차원(예를 들면, 2차원)으로 줄이는 방법
=> Word2Vec단어벡터들을 시각화 시켜주기 위해 t-SNE를 사용할 수 있다.

2. 의미적/문법적 정보 함축
임베딩은 벡터화 시켜주는 것이기 때문에 사칙연산이 가능
따라서, 단어 벡터간 뎃셈, 뺄셈을 통해 단어 사이의 의미적, 문법적 관계 도출 가능 
단어 유추 평가 : 단어 임베딩을 평가하는 방법

3. 전이 학습
임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 의미
임베딩의 품질이 좋으면 문서 분류 정확도와 학습 속도가 올라간다.

임베딩 기법의 역사와 종류
1. 통계기반에서 뉴럴 네트워크 기반으로
<통계기반>
초기 임베딩 기법은 대부분 말뭉치의 통계량을 직접적으로 활용하는 경향
잠재의미분석(LSA) : 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 커다란 행렬에 특이값 분해등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법
희소행렬 : 대붑분의 요소 값이 0인 행렬
<뉴럴 네트워크 기반>
뉴럴 네트워크 기반 모델들은 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측핳거나 문장 내 일부분에 구멍을 뚫어 놓고 해당 단어가 무엇일지 맞추는 과정에서 학습된다.

2. 단어수준에서 문장수준으로
[단어 수준의 임베딩 모델]
NPLM(Neural Probabilistic Language Model), Word2Vec, Glove, FastText, Swivel등
단어 임베딩 기법들은 각가의 벡터에 해당 단어의 문맥적 의미를 함축한다.
=> (단점) 동음이의어를 분간하기 어려움
       ∵ 단어의 형태가 같다면 동일한 단어로 보고 모든 문맥 정보를 해당 단어 벡터에 투영하기 때문에
[문장 수준의 임베딩 모델]
ELMo(Embeddings from Language Models), BERT(Bidirectional Encoder Representations from Transformer), GPT(Generative Pre-Training)등
개별단어가 아닌 단어 시퀀스 젠체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습 효과가 더 좋다.
문장 수준의 임베딩 기법은 동음이의어를 분간해 낸다.

3. 룰 ⇒ 엔드투엔드 ⇒ 프리트레인/파인 튜닝
룰 : 사람이 직접 피처(feature, 모델의 입력값)을 뽑았다.  
       피처추출은 언어학적인 지식을 활용
엔드투엔드 모델 : 데이터를 통째로 딥러닝 모델에 넣고 입출력 사이의 관계를 사람이 개입없이 모델 스스로 처음부터 끝까지 이해하도록 유도
                              예시 : 시쿼스투시권스 모델
프리트레인/파인 튜닝 :  대규모 말뭉치로 임베딩을 만든다. (프리트레인)
                                        이 임베딩에는 말뭉치의 의미적. 문법적 맥락이 포함되어 있다.
                                         임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들어 우리가 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트 한다.(파인 투닝, 전이 학습)
                                        ELMo, GPT, BERT등이 이 방식에 해당
