임베딩에 자연어 의미를 함축하는 방법은
자연어의 통게적 패턴 정보를 통째로 임베딩에 넣는 것

[ 임베딩을 만들 때 쓰는 통계정보 3가지 ]
1. 어떤 단어가 (많이) 쓰였는가  - 백오브워즈 가정
    대표적 통계량 : TF-IDF
    대표모델 : Deep Averaging Network  : 문장의 임베딩은 중복집합에 속한 단어의 임베딩들의 평균을 내서 만든다.
    단점 : 단어의 순서 무시
    ⇒ 보완점 :언어 모델 : 단어 시퀀스에 확률을 부여하는 모델, 백오브워즈와 달리 시퀀스 정보를 명시적으로 학습하여 한 단어 시퀀스 다음 단어는 무엇이 오는 게 자연스러운지 알 수 있다. (확률이 높을수록 더 자연스러운 문장)
2. 단어가 어떤 순서로 쓰였는가 - 언어 모델
대표모델 : ELMo, GPT
[ 통계 기반 언어 모델 ]
     n-gram : n개의 단어를 뜻하는 용어 로서 말뭉치 내의 단어들을 순서대로 n개씩 묶어서 그 빈도를 학습하는 것을 의미 

     특정 말뭉치 이후 다음 단어가 나올 확률은 조건부 확률을 활용한 최대우도추정법을 유도하면 된다.
      
예를 들면, '나는 오늘 치킨을 '다음 표현으로 '먹었다'라는 단어가 나타낼 확률을 구한다고 하자.
P( 먹었다 | 나는, 오늘, 치킨을 ) = Freq (나는, 오늘, 치킨을, 먹었다) / Freq(나는, 오늘, 치킨을)
                                   (바이그램일 경우) = P( 나는 ) * P( 오늘 | 나는 ) * P( 치킨을 | 오늘 ) * P( 먹었다 | 치킨을 ) 
                                                                       where P(나는) = ( '나는'이 나온 개수 ) / ( 어휘 집합에 속한 단어 수 )

 하지만 여기서 만약 그전에 '나는' 다음에 '오늘' 이란 단어가 오는 문장이 없었을 경우 P(오늘 | 나는) = 0 이 나온다는 문제가 있다.
이를 해결하기 위해 백오프, 스무딩 등의 방식이 존재한다.
백오프 : n-gram 등장 빈도를 n보다 잣은 범위의 단어 시퀀스 빈도로 근사하는 방식
             ∵  n을 크게 하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높기 때문이다.
            예시 :  Freq (나는, 오늘, 치킨을, 먹었다) ≒  a*Freq(치킨을, 먹었다) + b  where a,b 는 실제 빈도와의 차이를 보정해주는 파라미터
스무딩 : 등장빈도 표에 모두 k를 더하는 방법
             예시 : 나는 오늘 치킨을 먹었다 의 빈도는 Freq (나는, 오늘, 치킨을, 먹었다) + k

[ 뉴럴 네트워크 기반 언어 모델 ]  대표모델 : ELMo, GPT
뉴럴 네트워크 기반의 언어 모델은 주어진 다어 시퀀스를 가지고 다음단어를 맞추는 과정에서 학습된다.         
학습이 완료되면, 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문장의 임베딩으로 활용한다.
 예시 : 발없는 말이 ===(언어모델)===> 천리

[ 마스크 언어 모델 ] 대표모델 :BERT
문장 중간에 마스크를 씌워 놓고 해당 마스크 위치에 어떤 단어가 올지 예측하는 모델
언어모델 기법은 단어를 순차적으로 입력받아 다음 단어를 맞춰야 하는 것이기 때문에 태생적으로 일방향
하지만 마스크 언어 모델 기반 기법은 문장 전체를 보고 중간에 있는 단어를 예측하는 것이므로 양방향 학습이 가능 
 예시 : 발없는 말이 [Mask] 간다 ===(언어모델)===>천리

3.어떤 단어가 같이 쓰였는가 - 분포 가정
자연어 처리에서 분포는 특정범의, 즉 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 가리킨다. 
개별 단어의 분포는 그 단어가 문장 내에 주로 어느 위치에 나타내는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라진다. 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면, 그 의미 또한 유사할 것이라는 것이 분포 가정의 전제이다.
   대표적 통계량 : PMI
         예시 PMI(더위, 여름) = Log (P(더위,여름) / (P(더위)*P(여름)))
    대표모델 : Word2Vec
